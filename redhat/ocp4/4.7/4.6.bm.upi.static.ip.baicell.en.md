# openshift 4.6.28 static-IP baremetal deployment, UPI mode

> :heavy_exclamation_mark: WARNING!!!  deployment methods used in this doc/notes, contains software/methods from 3rd parties, like helper node, and something like static-IP, disable dhcp/tftp, and so one, you CAN'T get support from RedHat for these stuff. USE WITH YOUR OWN RISK. Good luck with you PoC. WARNING!!!

This doc describe how to install ocp4.6.28 on baremetal in UPI mode. Because the dhcp/tftp is forbiddend in most PoC env, we will use 3rd part software to deal with static-IP. We also assume the env can connect to internet directly.

![](dia/4.7.real-time.kernel.drawio.svg)

## install materal download

The install materal is pre-packaged, you can download it hereï¼Œit is for openshift 4.6.28:
- link: https://pan.baidu.com/s/1XFbiOAcz7nul-N9U0aDxHg  password: 6qtt

It includes such files:
- ocp4.tgz  this file contains iso files, installation scripts, image link list and so one. You need to copy this file to helper node.
- registry.tgz  this is the docker registry directory zip file, you can use this to setup openshift image installation image registry.

### if you want to create the install material file by yourself

how to prepare the installation file, you can check out the sepereated doc here: [4.7.build.dist.md](4.7.build.dist.md)

## prepare the kvm host

The PoC use machines with 96G memory, and 24C cpu, we will use one of them as kvm host, to host several kvm. We need to prepare the kvm host first.

Remember, you have to sync time between all of the machines, including your labtop. The difference of time should be less than 10 mins, otherwise, you will have certification expiration issues.

Some of the preparation including:
- configure dns
- install docker registry
- configure vnc (optional)
- configure network for kvm
- create helper vm

Above all, the configuration of dns, need to adjust, based on the PoC env.

We assume you kvm host already have rhel8 installed.

## kvm host prepartion
```bash
# we will enable epel, which is for convenience.
dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm

# configure the vnc env, it is also for convenience
dnf -y groupinstall "Server with GUI"
systemctl set-default graphical.target
systemctl get-default
# graphical.target

dnf -y install tigervnc-server

# set up the vnc password
vncpasswd

# setup vnc startup config
cat << EOF > ~/.vnc/config
session=gnome
securitytypes=vncauth,tlsvnc
# desktop=sandbox
geometry=1280x800
alwaysshared
EOF

# set root to be the user 1
cat << EOF >> /etc/tigervnc/vncserver.users
:1=root
EOF

# start root vnc desktop 
# then you can connect vnc using client, and @5901 tcp port
systemctl start vncserver@:1
# in case if you want to stop the vnc desktop
systemctl stop vncserver@:1

# configure kvm env
dnf -y install qemu-kvm libvirt libguestfs-tools virt-install virt-viewer virt-manager

# we disable firewalld for easy config, if you poc env needs firewalld, you have to configure firewall rules carefully.
systemctl disable --now firewalld
systemctl enable --now libvirtd

# create kvm network, we create a bridge network
# you need to change the ip parameters based on you PoC env.

mkdir -p /data/kvm
cat << 'EOF' > /data/kvm/bridge.sh
#!/usr/bin/env bash

PUB_CONN='eno1'
PUB_IP='172.21.6.104/24'
PUB_GW='172.21.6.254'
PUB_DNS='172.21.1.1'

nmcli con down "$PUB_CONN"
nmcli con delete "$PUB_CONN"
nmcli con down baremetal
nmcli con delete baremetal
# RHEL 8.1 appends the word "System" in front of the connection,delete in case it exists
nmcli con down "System $PUB_CONN"
nmcli con delete "System $PUB_CONN"
nmcli connection add ifname baremetal type bridge con-name baremetal ipv4.method 'manual' \
    ipv4.address "$PUB_IP" \
    ipv4.gateway "$PUB_GW" \
    ipv4.dns "$PUB_DNS"
    
nmcli con add type bridge-slave ifname "$PUB_CONN" master baremetal
nmcli con down "$PUB_CONN";pkill dhclient;dhclient baremetal
nmcli con up baremetal
EOF
bash /data/kvm/bridge.sh

# create the helper kvm

mkdir -p /data/kvm
cd /data/kvm

# try to find newest kvm parameters
osinfo-query os | grep rhel8
#  rhel8-unknown        | Red Hat Enterprise Linux 8 Unknown                 | 8-unknown | http://redhat.com/rhel/8-unknown
#  rhel8.0              | Red Hat Enterprise Linux 8.0                       | 8.0      | http://redhat.com/rhel/8.0
#  rhel8.1              | Red Hat Enterprise Linux 8.1                       | 8.1      | http://redhat.com/rhel/8.1
#  rhel8.2              | Red Hat Enterprise Linux 8.2                       | 8.2      | http://redhat.com/rhel/8.2
#  rhel8.3              | Red Hat Enterprise Linux 8.3                       | 8.3      | http://redhat.com/rhel/8.3
#  rhel8.4              | Red Hat Enterprise Linux 8.4                       | 8.4      | http://redhat.com/rhel/8.4

# you have to download rhel-8.4-x86_64-dvd.iso by youself in advance.

# prepare a dir for kvm data file
# it should be different, you have to configure the data dir based on your env.
lvcreate -y -l 100%FREE -n data nvme
mkfs.xfs /dev/nvme/data
mkdir -p /data/kvm
mount /dev/nvme/data /data/kvm

cat << EOF >> /etc/fstab
/dev/nvme/data /data/kvm                   xfs     defaults        0 0
EOF

cd /data/kvm

wget https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.7/scripts/helper-ks-rhel8.cfg

# change the ip address configuration of your helper kvm
# adjust the ip address information based on your PoC env.
sed -i '0,/^network.*/s/^network.*/network  --bootproto=static --device=enp1s0 --gateway=172.21.6.254 --ip=172.21.6.11  --netmask=255.255.255.0 --nameserver=172.21.1.1  --ipv6=auto --activate/' helper-ks-rhel8.cfg

virt-install --name="ocp4-aHelper" --vcpus=2 --ram=4096 \
--cpu=host-model \
--disk path=/data/kvm/ocp4-aHelper.qcow2,bus=virtio,size=150 \
--os-variant rhel8.4 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59000 \
--boot menu=on --location /data/kvm/rhel-8.4-x86_64-dvd.iso \
--initrd-inject helper-ks-rhel8.cfg --extra-args "inst.ks=file:/helper-ks-rhel8.cfg" 

# virt-viewer --domain-name ocp4-aHelper
# virsh start ocp4-aHelper
# virsh list --all

```

## config the helper vm

the config tasks include:
- config and run ansible scripts, to auto config the helper vm
- create ignition files

```bash
# we also use epel-release, although it is not a mandentory.
dnf install -y epel-release
dnf install -y byobu
dnf update -y
reboot

sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config
systemctl restart sshd

cat << EOF > /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

# config a chronyd source
echo "allow 192.0.0.0/8" >> /etc/chrony.conf
systemctl enable --now chronyd
# systemctl restart chronyd
chronyc tracking
chronyc sources -v
chronyc sourcestats -v
chronyc makestep

# on helper vm, add second ip address
# this is not mandentory, you have to change this based on you env
nmcli con mod enp1s0 +ipv4.addresses "192.168.7.11/24"
nmcli con up enp1s0

dnf -y install ansible git unzip podman python3 buildah skopeo

mkdir -p /data/ocp4/
# copy ocp4.tgz and registry.tgz to helper kvm
# scp ocp4.tgz to /data
# scp * root@172.21.6.11:/data/
cd /data
tar zvxf ocp4.tgz
tar zvxf registry.tgz
cd /data/ocp4

rm -f /data/*.tgz

# config registry
mkdir -p /etc/crts/ && cd /etc/crts

# https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.1/html/installation_guide/installing-codeready-workspaces-in-tls-mode-with-self-signed-certificates_crw
openssl genrsa -out /etc/crts/redhat.ren.ca.key 4096
openssl req -x509 \
  -new -nodes \
  -key /etc/crts/redhat.ren.ca.key \
  -sha256 \
  -days 36500 \
  -out /etc/crts/redhat.ren.ca.crt \
  -subj /CN="Local Red Hat Ren Signer" \
  -reqexts SAN \
  -extensions SAN \
  -config <(cat /etc/pki/tls/openssl.cnf \
      <(printf '[SAN]\nbasicConstraints=critical, CA:TRUE\nkeyUsage=keyCertSign, cRLSign, digitalSignature'))

openssl genrsa -out /etc/crts/redhat.ren.key 2048

openssl req -new -sha256 \
    -key /etc/crts/redhat.ren.key \
    -subj "/O=Local Red Hat Ren /CN=*.ocp4.redhat.ren" \
    -reqexts SAN \
    -config <(cat /etc/pki/tls/openssl.cnf \
        <(printf "\n[SAN]\nsubjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth")) \
    -out /etc/crts/redhat.ren.csr

openssl x509 \
    -req \
    -sha256 \
    -extfile <(printf "subjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth") \
    -days 365 \
    -in /etc/crts/redhat.ren.csr \
    -CA /etc/crts/redhat.ren.ca.crt \
    -CAkey /etc/crts/redhat.ren.ca.key \
    -CAcreateserial -out /etc/crts/redhat.ren.crt

openssl x509 -in /etc/crts/redhat.ren.crt -text

/bin/cp -f /etc/crts/redhat.ren.ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

cd /data
# mkdir -p /data/registry
# tar zxf registry.tgz
dnf -y install podman pigz skopeo jq 
# pigz -dc registry.tgz | tar xf -
cd /data/ocp4
podman load -i /data/ocp4/registry.tgz

systemctl disable --now firewalld

podman run --restart always --name local-registry -p 5443:5443 \
  -d --restart=always \
  -v /data/registry/:/var/lib/registry:z \
  -v /etc/crts:/certs:z \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:5443 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/redhat.ren.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/redhat.ren.key \
  docker.io/library/registry:2

# podman start local-registry

# we use a customized ansible project, to deploy service to helper kvm
# https://github.com/wangzheng422/ocp4-upi-helpernode this is used for configure the helper node, which provide some basic service for openshift, I make some changes/enhancement. 
# The upstream is here: https://github.com/RedHatOfficial/ocp4-helpernode
unzip ocp4-upi-helpernode.zip

# create a ssh key on helper kvm
ssh-keygen

# next, we will configure the ansible script, to install service to helper kvm
cd /data/ocp4/ocp4-upi-helpernode-master

cat << 'EOF' > /data/ocp4/ocp4-upi-helpernode-master/vars.yaml
---
ssh_gen_key: false  # we created the ssh key
staticips: true     # most PoC env does not allow dhcp, so enable static ip mode
bm_ipi: false       # we do not use IPI mode
firewalld: false
dns_forward: false  # set to true if you have public internet connection
iso:
  iso_dl_url: "file:///data/ocp4/rhcos-live.x86_64.iso" # leave as it is
  my_iso: "rhcos-live.iso"  # leave as it is
helper:
  name: "helper"
  ipaddr: "192.168.7.11"
  networkifacename: "enp1s0"  # the ifname inside the kvm
  gateway: "192.168.7.1"
  netmask: "255.255.255.0"
dns:
  domain: "redhat.ren"    # change based on your env
  clusterid: "ocp4"       # change based on you env
  forwarder1: "202.106.0.20"  # if you have public internet connection, set to 8.8.8.8, for example.
  forwarder2: "202.106.0.20"  # if you have public internet connection, set to 8.8.4.4, for example.
  api_vip: "192.168.7.11"     # only need if you use IPI mode
  ingress_vip: "192.168.7.11" # only need if you use IPI mode
dhcp:
  router: "192.168.7.1"     # this section no used, in UPI mode
  bcast: "192.168.7.255"
  netmask: "255.255.255.0"
  poolstart: "192.168.7.70"
  poolend: "192.168.7.90"
  ipid: "192.168.7.0"
  netmaskid: "255.255.255.0"
bootstrap:
  name: "bootstrap"
  ipaddr: "192.168.7.12"
  interface: "enp1s0"           # the ifname inside the kvm
  install_drive: "vda"
  macaddr: "52:54:00:7e:f8:f7"  # only needed in IPI mode
masters:
  - name: "master-0"
    ipaddr: "192.168.7.13"
    interface: "enp1s0"
    install_drive: "vda"
    macaddr: ""
  - name: "master-1"
    ipaddr: "192.168.7.14"
    interface: "enp1s0"
    install_drive: "vda"
    macaddr: ""
  - name: "master-2"
    ipaddr: "192.168.7.15"
    interface: "enp1s0"
    install_drive: "vda"
    macaddr: ""
workers:
  - name: "worker-0"
    ipaddr: "192.168.7.16"
    interface: "ens18f0"
    install_drive: "sda"
    macaddr: ""
  - name: "worker-1"
    ipaddr: "192.168.7.17"
    interface: "enp1s0"
    install_drive: "vda"
    macaddr: ""
others:
  - name: "registry"              # only needed if you want to add configuration to named.
    ipaddr: "192.168.7.1"
    macaddr: "52:54:00:7e:f8:f7"  # no used in UPI mode.
  - name: "yum"
    ipaddr: "192.168.7.1"
    macaddr: "52:54:00:7e:f8:f7"
  - name: "quay"
    ipaddr: "192.168.7.1"
    macaddr: "52:54:00:7e:f8:f7"
  - name: "nexus"
    ipaddr: "192.168.7.1"
    macaddr: "52:54:00:7e:f8:f7"
  - name: "git"
    ipaddr: "192.168.7.1"
    macaddr: "52:54:00:7e:f8:f7"
otherdomains:
  - domain: "rhv.redhat.ren"    # no use, if you have public internet connection, only needed if you want to add configuration to named.
    hosts:
    - name: "manager"
      ipaddr: "192.168.7.71"
    - name: "rhv01"
      ipaddr: "192.168.7.72"
  - domain: "cmri-edge.redhat.ren"
    hosts:
    - name: "*"
      ipaddr: "192.168.7.71"
    - name: "*.apps"
      ipaddr: "192.168.7.72"
force_ocp_download: false
remove_old_config_files: false
ocp_client: "file:///data/ocp4/4.6.28/openshift-client-linux-4.6.28.tar.gz"
ocp_installer: "file:///data/ocp4/4.6.28/openshift-install-linux-4.6.28.tar.gz"
ppc64le: false
arch: 'x86_64'
chronyconfig:
  enabled: true
  content:
    - server: "192.168.7.1"
      options: iburst
setup_registry:
  deploy: false
  registry_image: docker.io/library/registry:2
  local_repo: "ocp4/openshift4"
  product_repo: "openshift-release-dev"
  release_name: "ocp-release"
  release_tag: "4.6.1-x86_64"
ocp_filetranspiler: "file:///data/ocp4/filetranspiler.tgz"
registry_server: "registry.ocp4.redhat.ren:5443"  # change this if you build registry server outside the helper kvm
EOF

# run ansible scripts to install services
ansible-playbook -e @vars.yaml tasks/main.yml

# try this to check the result:
/usr/local/bin/helpernodecheck

# create dir to continue config ocp cluster
mkdir -p /data/install

# customize the ignition
cd /data/install

# you have to download your pull-secret.json to /data/pull-secret.json
# if you don't understand what it is, you have to check https://github.com/wangzheng422/docker_env/blob/dev/redhat/ocp4/4.5/4.5.ocp.pull.secret.md
# official url: https://console.redhat.com/openshift/install/pull-secret

# vi install-config.yaml 
cat << EOF > /data/install/install-config.yaml 
apiVersion: v1
baseDomain: redhat.ren
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 0
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 3
metadata:
  name: ocp4
networking:
  clusterNetworks:
  - cidr: 10.254.0.0/16
    hostPrefix: 24
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
pullSecret: '$( cat /data/pull-secret.json )'
sshKey: |
$( cat /root/.ssh/id_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /etc/crts/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

cd /data/install/
/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] 

openshift-install create manifests --dir=/data/install

# copy ntp related config
/bin/cp -f  /data/ocp4/ocp4-upi-helpernode-master/machineconfig/* /data/install/openshift/

cd /data/install/
openshift-install create ignition-configs --dir=/data/install

cd /data/ocp4/ocp4-upi-helpernode-master
# create ignition for each node, and copy to web server's dir
ansible-playbook -e @vars.yaml tasks/ign.yml

# create iso file for each node
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars.yaml tasks/iso.yml

```
## back to kvm host

```bash
# on kvm host 172.21.6.101
# copy back the iso files
export KVM_DIRECTORY=/home/data/kvm

mkdir -p  ${KVM_DIRECTORY}
cd ${KVM_DIRECTORY}
scp root@192.168.7.11:/data/install/{*boot*,*master-*,*worker-0}.iso ${KVM_DIRECTORY}/

# finally, we can start install :)
# you can create the kvm all, and wait with a cup of coffee
# from now on, it will cost about 30 mins.
export KVM_DIRECTORY=/home/data/kvm
virt-install --name=ocp4-bootstrap --vcpus=4 --ram=8192 \
--disk path=${KVM_DIRECTORY}/ocp4-bootstrap.qcow2,bus=virtio,size=50 \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59001 \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-bootstrap.iso   

# if you want to login into the coreos
# ssh core@bootstrap 
# journalctl -b -f -u bootkube.service

# it is recommand 6C, 20G for each master/worker node,
# but baicell using 4C 16G for each, adjust based on you env.
export KVM_DIRECTORY=/home/data/kvm
virt-install --name=ocp4-master-0 --vcpus=6 --ram=16384 \
--cpu=host-model \
--disk path=${KVM_DIRECTORY}/ocp4-master-0.qcow2,bus=virtio,size=120 \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59002 \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-0.iso 

export KVM_DIRECTORY=/home/data/kvm
virt-install --name=ocp4-master-1 --vcpus=6 --ram=16384 \
--cpu=host-model \
--disk path=${KVM_DIRECTORY}/ocp4-master-1.qcow2,bus=virtio,size=120 \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59003 \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-1.iso 

export KVM_DIRECTORY=/home/data/kvm
virt-install --name=ocp4-master-2 --vcpus=6 --ram=16384 \
--cpu=host-model \
--disk path=${KVM_DIRECTORY}/ocp4-master-2.qcow2,bus=virtio,size=120 \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59004 \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-2.iso 

# if you want to stop or delete vm, try this
virsh list --all
virsh destroy ocp4-bootstrap
virsh destroy ocp4-master-0 
virsh destroy ocp4-master-1 
virsh destroy ocp4-master-2 
# virsh destroy ocp4-worker0 
# virsh destroy ocp4-worker1 
# virsh destroy ocp4-worker2
virsh undefine ocp4-bootstrap --remove-all-storage
virsh undefine ocp4-master-0 --remove-all-storage
virsh undefine ocp4-master-1 --remove-all-storage
virsh undefine ocp4-master-2 --remove-all-storage
# virsh undefine ocp4-worker0 
# virsh undefine ocp4-worker1 
# virsh undefine ocp4-worker2

```

## back to helper kvm

The installation is automatically, you just wait on helper kvm.

you can check the progress of bootstrap:
```bash
cd /data/install
export KUBECONFIG=/data/install/auth/kubeconfig
echo "export KUBECONFIG=/data/install/auth/kubeconfig" >> ~/.bashrc
oc completion bash | sudo tee /etc/bash_completion.d/openshift > /dev/null

cd /data/install
openshift-install wait-for bootstrap-complete --log-level debug

```
if everything ok, the output will be like this:
![](../imgs/2019-10-23-12-17-30.png)

some times, you need to check csr, and approve it by yourself, under most cases, you don't need to do this.
```bash
oc get csr
```
if you find any pending....
![](../imgs/2019-10-22-15-19-58.png)

approve it
```bash
yum -y install jq
oc get csr | grep -v Approved
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve
# oc get csr -o name | xargs oc adm certificate approve

oc get node
```
wait a moment, you can see the worker node, like this:
![](../imgs/2019-10-22-15-23-08.png)

then, you wait until finish... and the output should be like this:
```bash
openshift-install wait-for install-complete --log-level debug
# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.redhat.ren
# INFO Login to the console with user: "kubeadmin", and password: "AM4Z2-tkLpm-gIUfM-wBSUC"

# we are testing env, so we don't need ingress replicas.
oc patch --namespace=openshift-ingress-operator --patch='{"spec": {"replicas": 1}}' --type=merge ingresscontroller/default

```

## install on baremetal

access the host's BMC, and mound the iso in the BMC interface, boot the machine, and the installation begins automatically.

In some cases, the installation can't begin, or failed, you have to boot the host using /data/ocp4/rhcos-live.x86_64.iso ( on helper node ), then configure the node manually, please [check here](./4.7.install.live.iso.md).

The offical documents is here: https://docs.openshift.com/container-platform/4.8/installing/installing_bare_metal/installing-bare-metal.html#installation-user-infra-machines-advanced_network_installing-bare-metal
