# Real-Time Kernel for Openshift4.6

本次试验部署架构图

![](dia/4.7.real-time.kernel.drawio.svg)

opened case for baiceill testing:
- https://access.redhat.com/support/cases/#/case/02991973
- https://bugzilla.redhat.com/show_bug.cgi?id=1984933

# 先使用performance addon operator，这个是官方推荐的方法。

 performance addon operator 是openshift4里面的一个operator，他的作用是，让用户进行简单的yaml配置，然后operator帮助客户进行复杂的kernel parameter, kubelet, tuned配置。

```bash

# install performance addon operator following offical document
# https://docs.openshift.com/container-platform/4.6/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html

cat << EOF > /data/install/pao-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-performance-addon-operator
  labels:
    openshift.io/run-level: "1"
EOF
oc create -f /data/install/pao-namespace.yaml

# then install pao in project openshift-performance-addon-operator

# then create mcp, be careful, the label must be there
cat << EOF > /data/install/worker-rt.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-rt
  labels:
    machineconfiguration.openshift.io/role: worker-rt
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-rt]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-rt: ""

EOF
oc create -f /data/install/worker-rt.yaml

# to restore
oc delete -f /data/install/worker-rt.yaml

oc label node worker-0 node-role.kubernetes.io/worker-rt=""

# 以下的配置，是保留了0-1核给系统，剩下的2-19核给应用。
cat << EOF > /data/install/performance.yaml
apiVersion: performance.openshift.io/v1
kind: PerformanceProfile
metadata:
   name: wzh-performanceprofile
spec:
  additionalKernelArgs:
    - no_timer_check
    - clocksource=tsc
    - tsc=perfect
    - selinux=0
    - enforcing=0
    - nmi_watchdog=0
    - softlockup_panic=0
    - isolcpus=2-19
    - nohz_full=2-19
    - idle=poll
    - default_hugepagesz=1G
    - hugepagesz=1G
    - hugepages=16
    - skew_tick=1
    - rcu_nocbs=2-19
    - kthread_cpus=0-1
    - irqaffinity=0-1
    - rcu_nocb_poll
    - iommu=pt
    - intel_iommu=on
    # profile creator
    - audit=0
    - idle=poll
    - intel_idle.max_cstate=0
    - mce=off
    - nmi_watchdog=0
    - nosmt
    - processor.max_cstate=1
  globallyDisableIrqLoadBalancing: true
  cpu:
      isolated: "2-19"
      reserved: "0-1"
  realTimeKernel:
      enabled: true
  numa:  
      topologyPolicy: "single-numa-node"
  nodeSelector:
      node-role.kubernetes.io/worker-rt: ""
  machineConfigPoolSelector:
    machineconfiguration.openshift.io/role: worker-rt
EOF
oc create -f /data/install/performance.yaml

# it will create following
  # runtimeClass: performance-wzh-performanceprofile
  # tuned: >-
  #   openshift-cluster-node-tuning-operator/openshift-node-performance-wzh-performanceprofile
# restore
oc delete -f /data/install/performance.yaml

# 一般都需要 cpu/numa 绑核，这个在 kubelet 的配置里面做
# cat << EOF > /data/install/cpumanager-kubeletconfig.yaml
# apiVersion: machineconfiguration.openshift.io/v1
# kind: KubeletConfig
# metadata:
#   name: cpumanager-enabled
# spec:
#   machineConfigPoolSelector:
#     matchLabels:
#       custom-kubelet: cpumanager-enabled
#   kubeletConfig:
#      cpuManagerPolicy: static 
#      cpuManagerReconcilePeriod: 5s 
#      topologyManagerPolicy: single-numa-node 
#      reservedSystemCPUs: "0,1" 
# EOF
# oc create -f  /data/install/cpumanager-kubeletconfig.yaml

# enable sctp
cat << EOF > /data/install/sctp-module.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 99-worker-rt-load-sctp-module
  labels:
    machineconfiguration.openshift.io/role: worker-rt
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
        - path: /etc/modprobe.d/sctp-blacklist.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,
        - path: /etc/modules-load.d/sctp-load.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,sctp
EOF
oc create -f /data/install/sctp-module.yaml

# check the result
ssh core@worker-0
uname -a
# Linux worker-0 4.18.0-193.51.1.rt13.101.el8_2.x86_64 #1 SMP PREEMPT RT Thu Apr 8 17:21:44 EDT 2021 x86_64 x86_64 x86_64 GNU/Linux

ps -ef | grep stalld
# root        4416       1  0 14:04 ?        00:00:00 /usr/local/bin/stalld -p 1000000000 -r 10000 -d 3 -t 20 --log_syslog --log_kmsg --foreground --pidfile /run/stalld.pid
# core        6601    6478  0 14:08 pts/0    00:00:00 grep --color=auto stalld

```
# create vBBU app
```yaml
---

# apiVersion: "k8s.cni.cncf.io/v1"
# kind: NetworkAttachmentDefinition
# metadata:
#   name: host-device-du
# spec:
#   config: '{
#     "cniVersion": "0.3.0",
#     "type": "host-device",
#     "device": "ens18f1",
#     "ipam": {
#       "type": "host-local",
#       "subnet": "192.168.12.0/24",
#       "rangeStart": "192.168.12.105",
#       "rangeEnd": "192.168.12.106",
#       "routes": [{
#         "dst": "0.0.0.0/0"
#       }],
#       "gateway": "192.168.12.1"
#     }
#   }'

apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: host-device-du
spec:
  config: '{
    "cniVersion": "0.3.0",
    "type": "host-device",
    "device": "ens18f1",
    "ipam": {
      "type": "static",
      "addresses": [ 
            {
              "address": "192.168.12.105/24"
            },
            {
              "address": "192.168.12.106/24"
            }
          ],
      "routes": [ 
          {
            "dst": "0.0.0.0/0", 
            "gw": "192.168.12.1"
          }
        ]
      }
    }'


---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: du-deployment1
  labels:
    app: du-deployment1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: du-pod1
  template:
    metadata:
      labels:
        app: du-pod1
      annotations:
        k8s.v1.cni.cncf.io/networks: '[
          { "name": "host-device-du",
            "interface": "veth11" }
          ]'
      cpu-load-balancing.crio.io: "true"
    spec:
      runtimeClassName: performance-wzh-performanceprofile
      containers:
      - name: du-container1
        image: "registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh-shell-03"
        imagePullPolicy: IfNotPresent
        tty: true
        stdin: true
        env:
          - name: duNetProviderDriver
            value: "host-netdevice"
        #command:
        #  - sleep
        #  - infinity
        securityContext:
            privileged: true
            capabilities:
                add:
                - CAP_SYS_ADMIN
        volumeMounts:
          - mountPath: /hugepages
            name: hugepage
          - name: lib-modules
            mountPath: /lib/modules
          - name: src
            mountPath: /usr/src
          - name: dev
            mountPath: /dev
          - name: cache-volume
            mountPath: /dev/shm
        resources:
          requests:
            cpu: 16
            memory: 48Gi
            hugepages-1Gi: 8Gi
          limits:
            cpu: 16
            memory: 48Gi
            hugepages-1Gi: 8Gi
      volumes:
        - name: hugepage
          emptyDir:
            medium: HugePages
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: src
          hostPath:
            path: /usr/src
        - name: dev
          hostPath:
            path: "/dev"
        - name: cache-volume
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      nodeSelector:
        node-role.kubernetes.io/worker-rt: ""

---
```

# trouble shooting
```bash
# the most important, kernel.sched_rt_runtime_us should be -1, it is setting for realtime, and for stalld
sysctl kernel.sched_rt_runtime_us

sysctl -w kernel.sched_rt_runtime_us=-1

ps -e -o uid,pid,ppid,cls,rtprio,pri,ni,cmd | grep 'stalld\|rcuc\|softirq\|worker\|bin_read\|dumgr\|duoam' 

oc adm must-gather \
--image=registry.redhat.io/openshift4/performance-addon-operator-must-gather-rhel8 

oc get performanceprofile wzh-performanceprofile -o yaml > wzh-performanceprofile.output.yaml

oc describe node/worker-0 > node.worker-0.output

oc describe mcp/worker-rt > mcp.worker-rt.output
```

# profile creator

https://docs.openshift.com/container-platform/4.8/scalability_and_performance/cnf-create-performance-profiles.html#cnf-about-the-profile-creator-tool_cnf-create-performance-profiles

```bash

oc adm must-gather --image=quay.io/openshift-kni/performance-addon-operator-must-gather:4.6-snapshot --dest-dir=must-gather

oc adm must-gather --image=quay.io/openshift-kni/performance-addon-operator-must-gather:4.8-snapshot --dest-dir=must-gather

mkdir -p /etc/containers/registries.conf.d

cp -f /data/ocp4/image.registries.conf /etc/containers/registries.conf.d/

podman run --entrypoint performance-profile-creator quay.io/openshift-kni/performance-addon-operator:4.8-snapshot -h

podman run --entrypoint performance-profile-creator -v /data/tmp/must-gather:/must-gather:z quay.io/openshift-kni/performance-addon-operator:4.8-snapshot --mcp-name=worker-rt --reserved-cpu-count=2 --topology-manager-policy=single-numa-node --rt-kernel=true --profile-name=wzh-performanceprofile --power-consumption-mode=ultra-low-latency --disable-ht=true  --must-gather-dir-path /must-gather > my-performance-profile.yaml

```

# baicell vdu container image build

```bash
cd /data/tmp

cat << 'EOF' > init.sh
#!/bin/bash

DU_PATH="/home/BaiBBU_SXSS/"
DRIVER_PATH="/home/bin/nr5g_img/rec/"
L1_PATH="/home/bin/nr5g_img/L1/"

# unset LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin/:/opt/intel/compilers_and_libraries_2018.1.176/linux/ipp/lib/intel64_lin/:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin/${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}


cd $DRIVER_PATH
./install.sh
sleep 5
cd $L1_PATH
./l1.sh -rsub6 &
sleep  30
cd $DU_PATH
./gNB_app start &

sleep infinity
EOF

cat << EOF > Dockerfile
FROM registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh-shell

RUN mkdir /data
COPY l1.sh /home/bin/nr5g_img/L1/l1.sh
RUN chmod +x /home/bin/nr5g_img/L1/l1.sh
COPY init.sh /data/init.sh
RUN chmod +x /data/init.sh

ENTRYPOINT "/data/init.sh"

EOF

buildah bud --squash -t registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh-shell-05 -f ./Dockerfile ./

buildah push registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh-shell-05


oc create -f /data/5gtest/flexran-hostdev-baicells.yaml

oc delete -f /data/5gtest/flexran-hostdev-baicells.yaml

oc create -f /home/wzh/k8s-conf/v-ran/cyclictestpod.wzh.yaml

oc delete -f /home/wzh/k8s-conf/v-ran/cyclictestpod.wzh.yaml

# check which cpu running on.
taskset -pc $$


```

## build from base image
```bash
podman run -it registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh bash

podman ps -a

buildah commit --squash d92b57138339 registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh

podman image push registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh

```

## remove worker-0
```bash
oc delete node worker-0


```

# DIY & check what is going on

!!!! Do not use DIY

```bash
cat << EOF > /data/install/05-worker-kernelarg-realtime.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker-rt
  name: 05-worker-kernelarg-realtime
spec:
  config:
    ignition:
      version: 3.1.0
  kernelArguments:
    - no_timer_check  # 禁止运行内核中时钟IRQ源缺陷检测代码。主要用于解决某些AMD平台的CPU占用过高以及时钟过快的故障。
    - clocksource=tsc # clocksource={jiffies|acpi_pm|hpet|tsc} tsc TSC(Time Stamp Counter)的主体是位于CPU里面的一个64位TSC寄存器，与传统的以中断形式存在的周期性时钟不同，TSC是以计数器形式存在的单步递增性时钟，两者的区别在于，周期性时钟是通过周期性触发中断达到计时目的，如心跳一般。而单步递增时钟则不发送中断，取而代之的是由软件自己在需要的时候去主动读取TSC寄存器的值来获得时间。TSC的精度更高并且速度更快，但仅能在较新的CPU(Sandy Bridge之后)上使用。
    - tsc=perfect
    - intel_pstate=disable  # intel_pstate驱动支持现代Intel处理器的温控。 intel_pstate=disable选项可以强制使用传统遗留的CPU驱动acpi_cpufreq
    - selinux=0
    - enforcing=0
    - nmi_watchdog=0  # 配置nmi_watchdog(不可屏蔽中断看门狗) 0 表示关闭看门狗；
    - softlockup_panic=0  # 是否在检测到软死锁(soft-lockup)的时候让内核panic
    - isolcpus=2-19 # 将列表中的CPU从内核SMP平衡和调度算法中剔除。 提出后并不是绝对不能再使用该CPU的，操作系统仍然可以强制指定特定的进程使用哪个CPU(可以通过taskset来做到)。该参数的目的主要是用于实现特定cpu只运行特定进程的目的。
    - nohz_full=2-19  #在 16 核的系统中，设定 nohz_full=1-15 可以在 1 到 15 内核中启用动态无时钟内核性能，并将所有的计时移动至唯一未设定的内核中（0 内核）, [注意](1)"boot CPU"(通常都是"0"号CPU)会无条件的从列表中剔除。(2)这里列出的CPU编号必须也要同时列进"rcu_nocbs=..."参数中。
    - idle=poll # 对CPU进入休眠状态的额外设置。poll 从根本上禁用休眠功能(也就是禁止进入C-states状态)，可以略微提升一些CPU性能，但是却需要多消耗许多电力，得不偿失。不推荐使用。
    - default_hugepagesz=1G
    - hugepagesz=1G
    - hugepages=32
    - skew_tick=1 # Offset the periodic timer tick per cpu to mitigate xtime_lock contention on larger systems, and/or RCU lock contention on all systems with CONFIG_MAXSMP set. Note: increases power consumption, thus should only be enabled if running jitter sensitive (HPC/RT) workloads.
    - rcu_nocbs=2-19  # 指定哪些CPU是No-CB CPU
    - kthread_cpus=0-1
    - irqaffinity=0-1 # 通过内核参数irqaffinity==[cpu列表],设置linux中断的亲和性，设置后，默认由这些cpu核来处理非CPU绑定中断。避免linux中断影响cpu2、cpu3上的实时应用，将linux中断指定到cpu0、cpu1处理。
    - rcu_nocb_poll # 减少了需要从卸载cpu执行唤醒操作。避免了rcuo kthreads线程显式的唤醒。另一方面这会增加耗电量
    - iommu=pt
    - intel_iommu=on
    # profile creator
    - audit=0
    - idle=poll
    - intel_idle.max_cstate=0
    - mce=off
    - nmi_watchdog=0
    - nosmt
    - processor.max_cstate=1
  kernelType: realtime
EOF
oc create -f /data/install/05-worker-kernelarg-realtime.yaml

cat << EOF > /data/install/cpumanager-kubeletconfig.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      machineconfiguration.openshift.io/role: worker-rt
  kubeletConfig:
     cpuManagerPolicy: static 
     cpuManagerReconcilePeriod: 5s 
     topologyManagerPolicy: single-numa-node 
     reservedSystemCPUs: "0,1" 
EOF
oc create -f  /data/install/cpumanager-kubeletconfig.yaml

oc get Tuned/openshift-node-performance-wzh-performanceprofile -o yaml -n openshift-cluster-node-tuning-operator | yq e '.spec.profile[0].data' - | sed  's|^|      |g' | sed '/include=openshift-node,/ s/$/,realtime/'

cat << EOF > /data/install/tuned.yaml
apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: wzh-realtime
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
`oc get Tuned/openshift-node-performance-wzh-performanceprofile -o yaml -n openshift-cluster-node-tuning-operator | yq e '.spec.profile[0].data' - | sed  's|^|      |g' | sed '/include=openshift-node,/ s/$/,realtime/'`
    name: wzh-realtime
  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: worker-rt
    priority: 10
    profile: wzh-realtime
EOF
oc create -f /data/install/tuned.yaml

oc delete -f /data/install/tuned.yaml

crictl logs `crictl ps --name tuned -o json | jq -r .containers[0].id`

crictl exec -it `crictl ps --name tuned -o json | jq -r .containers[0].id` bash

```

```bash
oc get runtimeclass
NAME                                 HANDLER            AGE
performance-wzh-performanceprofile   high-performance   43h

oc get KubeletConfig

oc get Tuned -n openshift-cluster-node-tuning-operator

oc get KubeletConfig/performance-wzh-performanceprofile -o yaml | yq e "del(.metadata.managedFields, .status, .metadata.creationTimestamp, .metadata.generation, .metadata.)" -
```
```yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  creationTimestamp: "2021-07-30T07:04:09Z"
  finalizers:
    - 99-worker-rt-generated-kubelet
  generation: 1
  name: performance-wzh-performanceprofile
  ownerReferences:
    - apiVersion: performance.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: PerformanceProfile
      name: wzh-performanceprofile
      uid: 6bfdf08f-3420-4ed2-997d-cfa5922031e6
  resourceVersion: "75507"
  selfLink: /apis/machineconfiguration.openshift.io/v1/kubeletconfigs/performance-wzh-performanceprofile
  uid: 5598e31f-142c-49d7-a29e-8da9b7080f20
spec:
  kubeletConfig:
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous: {}
      webhook:
        cacheTTL: 0s
      x509: {}
    authorization:
      webhook:
        cacheAuthorizedTTL: 0s
        cacheUnauthorizedTTL: 0s
    cpuManagerPolicy: static
    cpuManagerReconcilePeriod: 5s
    evictionPressureTransitionPeriod: 0s
    fileCheckFrequency: 0s
    httpCheckFrequency: 0s
    imageMinimumGCAge: 0s
    kind: KubeletConfiguration
    kubeReserved:
      cpu: 1000m
      memory: 500Mi
    nodeStatusReportFrequency: 0s
    nodeStatusUpdateFrequency: 0s
    reservedSystemCPUs: 0-1
    runtimeRequestTimeout: 0s
    streamingConnectionIdleTimeout: 0s
    syncFrequency: 0s
    systemReserved:
      cpu: 1000m
      memory: 500Mi
    topologyManagerPolicy: single-numa-node
    volumeStatsAggPeriod: 0s
  machineConfigPoolSelector:
    matchLabels:
      machineconfiguration.openshift.io/role: worker-rt
```


```bash
oc get Tuned/openshift-node-performance-wzh-performanceprofile -o yaml -n openshift-cluster-node-tuning-operator | yq e '.spec.profile[0].data' -


[main]
summary=Openshift node optimized for deterministic performance at the cost of increased power consumption, focused on low latency network performance. Based on Tuned 2.11 and Cluster node tuning (oc 4.5)
include=openshift-node,cpu-partitioning

# Inheritance of base profiles legend:
# cpu-partitioning -> network-latency -> latency-performance
# https://github.com/redhat-performance/tuned/blob/master/profiles/latency-performance/tuned.conf
# https://github.com/redhat-performance/tuned/blob/master/profiles/network-latency/tuned.conf
# https://github.com/redhat-performance/tuned/blob/master/profiles/cpu-partitioning/tuned.conf

# All values are mapped with a comment where a parent profile contains them.
# Different values will override the original values in parent profiles.

[variables]
# isolated_cores take a list of ranges; e.g. isolated_cores=2,4-7

isolated_cores=2-19


not_isolated_cores_expanded=${f:cpulist_invert:${isolated_cores_expanded}}

[cpu]
force_latency=cstate.id:1|3                   #  latency-performance  (override)
governor=performance                          #  latency-performance
energy_perf_bias=performance                  #  latency-performance
min_perf_pct=100                              #  latency-performance


[service]
service.stalld=start,enable


[vm]
transparent_hugepages=never                   #  network-latency

[scheduler]
group.ksoftirqd=0:f:11:*:ksoftirqd.*
group.rcuc=0:f:11:*:rcuc.*

[sysctl]
kernel.hung_task_timeout_secs = 600           # cpu-partitioning #realtime
kernel.nmi_watchdog = 0                       # cpu-partitioning #realtime
kernel.sched_rt_runtime_us = -1               # realtime
kernel.timer_migration = 0                    # cpu-partitioning (= 1) #realtime (= 0)
kernel.numa_balancing=0                       # network-latency
net.core.busy_read=50                         # network-latency
net.core.busy_poll=50                         # network-latency
net.ipv4.tcp_fastopen=3                       # network-latency
vm.stat_interval = 10                         # cpu-partitioning  #realtime

# ktune sysctl settings for rhel6 servers, maximizing i/o throughput
#
# Minimal preemption granularity for CPU-bound tasks:
# (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)
kernel.sched_min_granularity_ns=10000000      # latency-performance

# If a workload mostly uses anonymous memory and it hits this limit, the entire
# working set is buffered for I/O, and any more write buffering would require
# swapping, so it's time to throttle writes until I/O can catch up.  Workloads
# that mostly use file mappings may be able to use even higher values.
#
# The generator of dirty data starts writeback at this percentage (system default
# is 20%)
vm.dirty_ratio=10                             # latency-performance

# Start background writeback (via writeback threads) at this percentage (system
# default is 10%)
vm.dirty_background_ratio=3                   # latency-performance

# The swappiness parameter controls the tendency of the kernel to move
# processes out of physical memory and onto the swap disk.
# 0 tells the kernel to avoid swapping processes out of physical memory
# for as long as possible
# 100 tells the kernel to aggressively swap processes out of physical memory
# and move them to swap cache
vm.swappiness=10                              # latency-performance

# The total time the scheduler will consider a migrated process
# "cache hot" and thus less likely to be re-migrated
# (system default is 500000, i.e. 0.5 ms)
kernel.sched_migration_cost_ns=5000000        # latency-performance

[selinux]
avc_cache_threshold=8192                      # Custom (atomic host)

[net]
nf_conntrack_hashsize=131072                  # Custom (atomic host)

[bootloader]
# set empty values to disable RHEL initrd setting in cpu-partitioning
initrd_remove_dir=
initrd_dst_img=
initrd_add_dir=
# overrides cpu-partitioning cmdline
cmdline_cpu_part=+nohz=on rcu_nocbs=${isolated_cores} tuned.non_isolcpus=${not_isolated_cpumask} intel_pstate=disable nosoftlockup

cmdline_realtime=+tsc=nowatchdog intel_iommu=on iommu=pt isolcpus=managed_irq,${isolated_cores} systemd.cpu_affinity=${not_isolated_cores_expanded}

cmdline_hugepages=+
cmdline_additionalArg=+ no_timer_check clocksource=tsc tsc=perfect selinux=0 enforcing=0 nmi_watchdog=0 softlockup_panic=0 isolcpus=2-19 nohz_full=2-19 idle=poll default_hugepagesz=1G hugepagesz=1G hugepages=16 skew_tick=1 rcu_nocbs=2-19 kthread_cpus=0-1 irqaffinity=0-1 rcu_nocb_poll iommu=pt intel_iommu=on


```


```bash
[ 1089.301838] stalld: ksoftirqd/11-112 starved on CPU 11 for 672 seconds
[ 1089.313775] stalld: boost_with_deadline failed to boost pid 112: Operation not permitted
[ 1089.327599] stalld: kworker/11:1-253 starved on CPU 11 for 666 seconds
[ 1089.339210] stalld: boost_with_deadline failed to boost pid 253: Operation not permitted
[ 1090.356234] stalld: ksoftirqd/11-112 starved on CPU 11 for 673 seconds
[ 1090.367902] stalld: boost_with_deadline failed to boost pid 112: Operation not permitted
[ 1090.381737] stalld: kworker/11:1-253 starved on CPU 11 for 667 seconds
[ 1090.393352] stalld: boost_with_deadline failed to boost pid 253: Operation not permitted
[ 1091.410439] stalld: ksoftirqd/11-112 starved on CPU 11 for 674 seconds
[ 1091.422614] stalld: boost_with_deadline failed to boost pid 112: Operation not permitted
[ 1091.436161] stalld: kworker/11:1-253 starved on CPU 11 for 668 seconds
[ 1091.447786] stalld: boost_with_deadline failed to boost pid 253: Operation not permitted
[ 1092.464859] stalld: ksoftirqd/11-112 starved on CPU 11 for 675 seconds
[ 1092.477063] stalld: boost_with_deadline failed to boost pid 112: Operation not permitted
[ 1092.490310] stalld: kworker/11:1-253 starved on CPU 11 for 669 seconds
[ 1092.501919] stalld: boost_with_deadline failed to boost pid 253: Operation not permitted
[ 1093.519044] stalld: ksoftirqd/11-112 starved on CPU 11 for 676 seconds
[ 1093.530693] stalld: boost_with_deadline failed to boost pid 112: Operation not permitted
[ 1093.544536] stalld: kworker/11:1-253 starved on CPU 11 for 670 seconds
[ 1093.556172] stalld: boost_with_deadline failed to boost pid 253: Operation not permitted
[ 1094.573213] stalld: ksoftirqd/11-112 starved on CPU 11 for 677 seconds
[ 1094.585859] stalld: boost_with_deadline failed to boost pid 112: Operation not permitted
[ 1094.599127] stalld: kworker/11:1-253 starved on CPU 11 for 671 seconds
[ 1094.610777] stalld: boost_with_deadline failed to boost pid 253: Operation not permitted




ps -e -o uid,pid,ppid,cls,rtprio,pri,ni,cmd | grep 'stalld\|rcuc\|softirq\|worker\|bin_read\|dumgr\|duoam'


    0       6       2  TS      -  39 -20 [kworker/0:0H]
    0       8       2  TS      -  19   0 [kworker/u40:0-events_unbound]
    0      10       2  FF     11  51   - [ksoftirqd/0]
    0      13       2  FF     11  51   - [rcuc/0]
    0      22       2  FF     11  51   - [rcuc/1]
    0      23       2  FF     11  51   - [ksoftirqd/1]
    0      25       2  TS      -  39 -20 [kworker/1:0H-kblockd]
    0      30       2  FF     11  51   - [rcuc/2]
    0      31       2  FF     11  51   - [ksoftirqd/2]
    0      32       2  TS      -  19   0 [kworker/2:0-mm_percpu_wq]
    0      33       2  TS      -  39 -20 [kworker/2:0H]
    0      39       2  FF     11  51   - [rcuc/3]
    0      40       2  FF     11  51   - [ksoftirqd/3]
    0      41       2  TS      -  19   0 [kworker/3:0-mm_percpu_wq]
    0      42       2  TS      -  39 -20 [kworker/3:0H]
    0      48       2  FF     11  51   - [rcuc/4]
    0      49       2  FF     11  51   - [ksoftirqd/4]
    0      50       2  TS      -  19   0 [kworker/4:0-mm_percpu_wq]
    0      51       2  TS      -  39 -20 [kworker/4:0H]
    0      57       2  FF     11  51   - [rcuc/5]
    0      58       2  FF     11  51   - [ksoftirqd/5]
    0      59       2  TS      -  19   0 [kworker/5:0-mm_percpu_wq]
    0      60       2  TS      -  39 -20 [kworker/5:0H]
    0      66       2  FF     11  51   - [rcuc/6]
    0      67       2  FF     11  51   - [ksoftirqd/6]
    0      68       2  TS      -  19   0 [kworker/6:0-mm_percpu_wq]
    0      69       2  TS      -  39 -20 [kworker/6:0H]
    0      75       2  FF     11  51   - [rcuc/7]
    0      76       2  FF     11  51   - [ksoftirqd/7]
    0      77       2  TS      -  19   0 [kworker/7:0-mm_percpu_wq]
    0      78       2  TS      -  39 -20 [kworker/7:0H]
    0      84       2  FF     11  51   - [rcuc/8]
    0      85       2  FF     11  51   - [ksoftirqd/8]
    0      86       2  TS      -  19   0 [kworker/8:0-mm_percpu_wq]
    0      87       2  TS      -  39 -20 [kworker/8:0H]
    0      93       2  FF     11  51   - [rcuc/9]
    0      94       2  FF     11  51   - [ksoftirqd/9]
    0      95       2  TS      -  19   0 [kworker/9:0-mm_percpu_wq]
    0      96       2  TS      -  39 -20 [kworker/9:0H]
    0     102       2  FF     11  51   - [rcuc/10]
    0     103       2  FF     11  51   - [ksoftirqd/10]
    0     104       2  TS      -  19   0 [kworker/10:0-mm_percpu_wq]
    0     105       2  TS      -  39 -20 [kworker/10:0H]
    0     111       2  FF     11  51   - [rcuc/11]
    0     112       2  FF     11  51   - [ksoftirqd/11]
    0     113       2  TS      -  19   0 [kworker/11:0-mm_percpu_wq]
    0     114       2  TS      -  39 -20 [kworker/11:0H]
    0     121       2  FF     11  51   - [rcuc/12]
    0     122       2  FF     11  51   - [ksoftirqd/12]
    0     123       2  TS      -  19   0 [kworker/12:0-mm_percpu_wq]
    0     124       2  TS      -  39 -20 [kworker/12:0H]
    0     130       2  FF     11  51   - [rcuc/13]
    0     131       2  FF     11  51   - [ksoftirqd/13]
    0     132       2  TS      -  19   0 [kworker/13:0-mm_percpu_wq]
    0     133       2  TS      -  39 -20 [kworker/13:0H]
    0     139       2  FF     11  51   - [rcuc/14]
    0     140       2  FF     11  51   - [ksoftirqd/14]
    0     141       2  TS      -  19   0 [kworker/14:0-mm_percpu_wq]
    0     142       2  TS      -  39 -20 [kworker/14:0H]
    0     148       2  FF     11  51   - [rcuc/15]
    0     149       2  FF     11  51   - [ksoftirqd/15]
    0     150       2  TS      -  19   0 [kworker/15:0-mm_percpu_wq]
    0     151       2  TS      -  39 -20 [kworker/15:0H]
    0     157       2  FF     11  51   - [rcuc/16]
    0     158       2  FF     11  51   - [ksoftirqd/16]
    0     159       2  TS      -  19   0 [kworker/16:0-mm_percpu_wq]
    0     160       2  TS      -  39 -20 [kworker/16:0H]
    0     166       2  FF     11  51   - [rcuc/17]
    0     167       2  FF     11  51   - [ksoftirqd/17]
    0     168       2  TS      -  19   0 [kworker/17:0-mm_percpu_wq]
    0     169       2  TS      -  39 -20 [kworker/17:0H]
    0     175       2  FF     11  51   - [rcuc/18]
    0     176       2  FF     11  51   - [ksoftirqd/18]
    0     177       2  TS      -  19   0 [kworker/18:0-mm_percpu_wq]
    0     178       2  TS      -  39 -20 [kworker/18:0H]
    0     184       2  FF     11  51   - [rcuc/19]
    0     185       2  FF     11  51   - [ksoftirqd/19]
    0     186       2  TS      -  19   0 [kworker/19:0-mm_percpu_wq]
    0     187       2  TS      -  39 -20 [kworker/19:0H]
    0     244       2  TS      -  19   0 [kworker/2:1-mm_percpu_wq]
    0     245       2  TS      -  19   0 [kworker/3:1-mm_percpu_wq]
    0     246       2  TS      -  19   0 [kworker/4:1-mm_percpu_wq]
    0     247       2  TS      -  19   0 [kworker/5:1-mm_percpu_wq]
    0     248       2  TS      -  19   0 [kworker/6:1-mm_percpu_wq]
    0     249       2  TS      -  19   0 [kworker/7:1-mm_percpu_wq]
    0     250       2  TS      -  19   0 [kworker/8:1-mm_percpu_wq]
    0     251       2  TS      -  19   0 [kworker/9:1-mm_percpu_wq]
    0     252       2  TS      -  19   0 [kworker/10:1-mm_percpu_wq]
    0     253       2  TS      -  19   0 [kworker/11:1-mm_percpu_wq]
    0     254       2  TS      -  19   0 [kworker/12:1-mm_percpu_wq]
    0     255       2  TS      -  19   0 [kworker/13:1-mm_percpu_wq]
    0     256       2  TS      -  19   0 [kworker/14:1-mm_percpu_wq]
    0     257       2  TS      -  19   0 [kworker/15:1-mm_percpu_wq]
    0     258       2  TS      -  19   0 [kworker/16:1-mm_percpu_wq]
    0     259       2  TS      -  19   0 [kworker/17:1-mm_percpu_wq]
    0     260       2  TS      -  19   0 [kworker/18:1-mm_percpu_wq]
    0     261       2  TS      -  19   0 [kworker/19:1-mm_percpu_wq]
    0     813       2  TS      -  19   0 [kworker/u40:7-events_unbound]
    0     850       2  TS      -  39 -20 [kworker/1:1H-kblockd]
    0    1180       2  TS      -  39 -20 [kworker/u41:0]
    0    2099       1  TS      -  19   0 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --runtime-cgroups=/system.slice/crio.service --node-labels=node-role.kubernetes.io/worker,node.openshift.io/os_id=rhcos --node-ip=192.168.7.16 --minimum-container-ttl-duration=6m0s --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --cloud-provider= --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aa6410430d05d37ac258c34c2279653595ea82dcaabab5ad1bd5c1eb8db14c39 --v=3
    0    2683    2655  TS      -  19   0 /usr/bin/openshift-sdn-node --node-name worker-0 --node-ip 192.168.7.16 --proxy-config /config/kube-proxy-config.yaml --v 2
    0    3953    3934  TS      -  19   0 /usr/bin/network-metrics --node-name worker-0
    0    4728       1  FF     10  50   - /usr/local/bin/stalld -p 1000000000 -r 10000 -d 3 -t 20 --log_syslog --log_kmsg --foreground --pidfile /run/stalld.pid
    0   14036   12919  TS      -  19   0 ./duoam ../cfg/Proprietary_gNodeB_DU_Data_Model.xml ../cfg/TR196_gNodeB_DU_Data_Model.xml
    0   14038   14036  TS      -  19   0 dumgr --dumgr_ip 192.168.12.105 --dumgr_port 2236 --duoam_ip 192.168.12.105 --duoam_port 2235 --shared_core_bitmap 28
    0   14039   14036  TS      -  19   0 gnb_du_layer2 --mac_ip=192.168.12.105 --mac_port=2336 --duoam_ip=192.168.12.105 --duoam_port=2235 --rlc_ip=192.168.12.105 --rlc_port=2436 --duf1u_ip=192.168.12.105 --duf1u_port=2536 --dumgr_ip=192.168.12.105 --dumgr_port=2236 --bin_reader_core=2 --log_num_files=20 --hp_core_num=5 --rlc_master_core_num=6 --shared_core_bitmap=28 --recvr_thread_core_num=9 --shm_size=50
    0   14040   14039  FF     80 120   - bin_reader --shm_name=MAC_REGION --shm_size=50 --log_file_name=MAC_REGION --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14144   14039  FF     80 120   - bin_reader --shm_name=RLC_TIMER_REGION_T0_ --shm_size=50 --log_file_name=RLC_TIMER_REGION_T0_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14146   14039  FF     80 120   - bin_reader --shm_name=RLC_ACCUMULATOR_REGION --shm_size=50 --log_file_name=RLC_ACCUMULATOR_REGION --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14148   14039  FF     80 120   - bin_reader --shm_name=PR_DU_REGION --shm_size=50 --log_file_name=PR_DU_REGION --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14149   14039  FF     80 120   - bin_reader --shm_name=RLC_MASTER_REGION --shm_size=50 --log_file_name=RLC_MASTER_REGION --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14169   14039  FF     80 120   - bin_reader --shm_name=F1_DU_WORKER_REGION_T0_ --shm_size=50 --log_file_name=F1_DU_WORKER_REGION_T0_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14171   14039  FF     80 120   - bin_reader --shm_name=RECEIVER_DU_REGION_T0_ --shm_size=50 --log_file_name=RECEIVER_DU_REGION_T0_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14174   14039  FF     80 120   - bin_reader --shm_name=RLC_WORKER_REGION_T0_ --shm_size=50 --log_file_name=RLC_WORKER_REGION_T0_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14196   14039  FF     80 120   - bin_reader --shm_name=MAC_SLOT_HDLR_THD_REGION_T0_ --shm_size=50 --log_file_name=MAC_SLOT_HDLR_THD_REGION_T0_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14198   14039  FF     80 120   - bin_reader --shm_name=MAC_TX_CTRL_DATA_THD_REGION_T0_ --shm_size=50 --log_file_name=MAC_TX_CTRL_DATA_THD_REGION_T0_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14200   14039  FF     80 120   - bin_reader --shm_name=MAC_LP_THD_REGION_T0_ --shm_size=50 --log_file_name=MAC_LP_THD_REGION_T0_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14562   14039  FF     80 120   - bin_reader --shm_name=MAC_SLOT_HDLR_THD_REGION_T1_ --shm_size=50 --log_file_name=MAC_SLOT_HDLR_THD_REGION_T1_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14564   14039  FF     80 120   - bin_reader --shm_name=MAC_TX_CTRL_DATA_THD_REGION_T1_ --shm_size=50 --log_file_name=MAC_TX_CTRL_DATA_THD_REGION_T1_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14566   14039  FF     80 120   - bin_reader --shm_name=MAC_LP_THD_REGION_T1_ --shm_size=50 --log_file_name=MAC_LP_THD_REGION_T1_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14709   14039  FF     80 120   - bin_reader --shm_name=MAC_SLOT_HDLR_THD_REGION_T2_ --shm_size=50 --log_file_name=MAC_SLOT_HDLR_THD_REGION_T2_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14711   14039  FF     80 120   - bin_reader --shm_name=MAC_TX_CTRL_DATA_THD_REGION_T2_ --shm_size=50 --log_file_name=MAC_TX_CTRL_DATA_THD_REGION_T2_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14723   14039  FF     80 120   - bin_reader --shm_name=MAC_LP_THD_REGION_T2_ --shm_size=50 --log_file_name=MAC_LP_THD_REGION_T2_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14872   14039  FF     80 120   - bin_reader --shm_name=MAC_SLOT_HDLR_THD_REGION_T3_ --shm_size=50 --log_file_name=MAC_SLOT_HDLR_THD_REGION_T3_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14874   14039  FF     80 120   - bin_reader --shm_name=MAC_TX_CTRL_DATA_THD_REGION_T3_ --shm_size=50 --log_file_name=MAC_TX_CTRL_DATA_THD_REGION_T3_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   14876   14039  FF     80 120   - bin_reader --shm_name=MAC_LP_THD_REGION_T3_ --shm_size=50 --log_file_name=MAC_LP_THD_REGION_T3_ --logger_option=binary --log_file_size=52428800 --bin_name=gnb_du_layer2 --log_num_files=20 --core_num=2
    0   20627       2  TS      -  19   0 [kworker/0:1-events_power_efficient]
    0   32565       2  TS      -  19   0 [kworker/u40:1-events_unbound]
    0   56215       2  TS      -  19   0 [kworker/1:1-cgroup_pidlist_destroy]
    0   65381       2  TS      -  19   0 [kworker/1:0-xfs-cil/dm-0]
    0   66247       2  TS      -  39 -20 [kworker/0:2H-kblockd]
    0   69121       2  TS      -  19   0 [kworker/u40:2-events_unbound]
    0   70399       2  TS      -  19   0 [kworker/0:0-events]
    0   74568       2  TS      -  19   0 [kworker/1:2-cgroup_pidlist_destroy]
    0   79426       2  TS      -  19   0 [kworker/0:2-events]
    0   83723    4863  TS      -  19   0 grep --color=auto stalld\|rcuc\|softirq\|worker\|bin_read\|dumgr\|duoam



```
