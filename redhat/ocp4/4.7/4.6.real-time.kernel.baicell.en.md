# Real-Time Kernel for Openshift4.6

This doc/notes is for deploy baicell vRAN/vDU on openshift 4.6.28. It includes enable rt-kernel and other stuff needed by baicell.

the deployment architecture:

![](dia/4.7.real-time.kernel.drawio.svg)

opened case for baiceill testing:
- https://access.redhat.com/support/cases/#/case/02991973
- https://bugzilla.redhat.com/show_bug.cgi?id=1984933

# pre-requirement
- a running openshift 4.6.28 cluster, with 1 baremetal worker node dedicated for the rt-kernel at least. double check openshift version requirement with baicell before PoC. 
- In baicell lab, the dedicated baremetal worker node have 20Core cpu, 128GB memory, 500GB ssd. double check the baremetal worker node capability requirement with baicell before PoC.
- hardware, like FPGA card, from baicell is on board.
- 5GC, RU, and so on, the network env is there. under most case, baicell will take care of them, double check with them.
- system admin with fine-tune background, RHCE is required at least, RH442 certification is recommended.
- k8s/openshift admin background, RH280 certification is recommended.
- read [performance addon operator offical documents](https://docs.openshift.com/container-platform/4.6/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html) carefully, and understand all of them.

# BIOS config for vRAN/vDU
If you want to run vRAN/vDU on openshift, you must enalbe rt-kernel on dedicate worker node, beside this, you must also set parameter in bios, the bios settings are different among servers, generally speaking, it include disable hyper-thread, disable irq balance, disable cpu c-state and other energy saving feature. [here is an example](https://www.dell.com/downloads/global/products/pedge/en/configuring_low_Latency_environments_on_dell_poweredge_servers.pdf)

| System Setup Screen | Setting           | Default                 | Recommended Alternative for Low- Latency Environments |
| ------------------- | ----------------- | ----------------------- | ----------------------------------------------------- |
| Processor Settings  | Logical Processor | Enabled                 | Disabled                                              |
| Processor Settings  | Turbo Mode        | Enabled                 | Disabled2                                             |
| Processor Settings  | C-States          | Enabled                 | Disabled                                              |
| Processor Settings  | C1E               | Enabled                 | Disabled                                              |
| Power Management    | Power Management  | Active Power Controller | Maximum Performance                                   |

# try performance addon operator firstï¼Œthis is offical recommended method

performance addon operator is an operator inside openshift4, his role is, let the user to do simple yaml configuration, then operator help customers to do complex kernel parameter, kubelet, tuned configuration.

```bash

# install performance addon operator following offical document
# https://docs.openshift.com/container-platform/4.6/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html

# then, following the offical document, config ocp step by step
cat << EOF > /data/install/pao-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-performance-addon-operator
  labels:
    openshift.io/run-level: "1"
EOF
oc create -f /data/install/pao-namespace.yaml

# then install pao in project openshift-performance-addon-operator

# then create mcp, be careful, the label must be there
cat << EOF > /data/install/worker-rt.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-rt
  labels:
    machineconfiguration.openshift.io/role: worker-rt
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-rt]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-rt: ""

EOF
oc create -f /data/install/worker-rt.yaml

# to restore
oc delete -f /data/install/worker-rt.yaml

oc label node worker-0 node-role.kubernetes.io/worker-rt=""

# The following configuration is reserved for 0-1 cores for the system and the remaining 2-19 cores for the application.
cat << EOF > /data/install/performance.yaml
apiVersion: performance.openshift.io/v1
kind: PerformanceProfile
metadata:
   name: wzh-performanceprofile
spec:
  additionalKernelArgs:
    - no_timer_check
    - clocksource=tsc
    - tsc=perfect
    - selinux=0
    - enforcing=0
    - nmi_watchdog=0
    - softlockup_panic=0
    - isolcpus=2-19
    - nohz_full=2-19
    - idle=poll
    - default_hugepagesz=1G
    - hugepagesz=1G
    - hugepages=16
    - skew_tick=1
    - rcu_nocbs=2-19
    - kthread_cpus=0-1
    - irqaffinity=0-1
    - rcu_nocb_poll
    - iommu=pt
    - intel_iommu=on
    # profile creator
    - audit=0
    - idle=poll
    - intel_idle.max_cstate=0
    - mce=off
    - nmi_watchdog=0
    - nosmt
    - processor.max_cstate=1
  globallyDisableIrqLoadBalancing: true
  cpu:
      isolated: "2-19"
      reserved: "0-1"
  realTimeKernel:
      enabled: true
  numa:  
      topologyPolicy: "single-numa-node"
  nodeSelector:
      node-role.kubernetes.io/worker-rt: ""
  machineConfigPoolSelector:
    machineconfiguration.openshift.io/role: worker-rt
EOF
oc create -f /data/install/performance.yaml

# it will create following
  # runtimeClass: performance-wzh-performanceprofile
  # tuned: >-
  #   openshift-cluster-node-tuning-operator/openshift-node-performance-wzh-performanceprofile

# restore
oc delete -f /data/install/performance.yaml

# enable sctp, which is a requirement from baicell vDU/intel ric.
cat << EOF > /data/install/sctp-module.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 99-worker-rt-load-sctp-module
  labels:
    machineconfiguration.openshift.io/role: worker-rt
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
        - path: /etc/modprobe.d/sctp-blacklist.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,
        - path: /etc/modules-load.d/sctp-load.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,sctp
EOF
oc create -f /data/install/sctp-module.yaml

# check the result
ssh core@worker-0
uname -a
# Linux worker-0 4.18.0-193.51.1.rt13.101.el8_2.x86_64 #1 SMP PREEMPT RT Thu Apr 8 17:21:44 EDT 2021 x86_64 x86_64 x86_64 GNU/Linux

ps -ef | grep stalld
# root        4416       1  0 14:04 ?        00:00:00 /usr/local/bin/stalld -p 1000000000 -r 10000 -d 3 -t 20 --log_syslog --log_kmsg --foreground --pidfile /run/stalld.pid
# core        6601    6478  0 14:08 pts/0    00:00:00 grep --color=auto stalld

```
# create vBBU app
This is the deployment yaml used to deploy baicell's vBBU app, it is a deploy with 1 replica, and only a big pod. It will use host-device to occupy a nic from host. And also run in privilidge mode, and mount /dev, to access the FPGA directly.
```yaml
---

apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: host-device-du
spec:
  config: '{
    "cniVersion": "0.3.0",
    "type": "host-device",
    "device": "ens18f1",
    "ipam": {
      "type": "host-local",
      "subnet": "192.168.12.0/24",
      "rangeStart": "192.168.12.105",
      "rangeEnd": "192.168.12.106",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.12.1"
    }
  }'

# apiVersion: "k8s.cni.cncf.io/v1"
# kind: NetworkAttachmentDefinition
# metadata:
#   name: host-device-du
# spec:
#   config: '{
#     "cniVersion": "0.3.0",
#     "type": "host-device",
#     "device": "ens18f1",
#     "ipam": {
#       "type": "static",
#       "addresses": [ 
#             {
#               "address": "192.168.12.105/24"
#             },
#             {
#               "address": "192.168.12.106/24"
#             }
#           ],
#       "routes": [ 
#           {
#             "dst": "0.0.0.0/0", 
#             "gw": "192.168.12.1"
#           }
#         ]
#       }
#     }'


---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: du-deployment1
  labels:
    app: du-deployment1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: du-pod1
  template:
    metadata:
      labels:
        app: du-pod1
      annotations:
        k8s.v1.cni.cncf.io/networks: '[
          { "name": "host-device-du",
            "interface": "veth11" }
          ]'
      cpu-load-balancing.crio.io: "true"
    spec:
      runtimeClassName: performance-wzh-performanceprofile
      containers:
      - name: du-container1
        image: "registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh-shell-03"
        imagePullPolicy: IfNotPresent
        tty: true
        stdin: true
        env:
          - name: duNetProviderDriver
            value: "host-netdevice"
        #command:
        #  - sleep
        #  - infinity
        securityContext:
            privileged: true
            capabilities:
                add:
                - CAP_SYS_ADMIN
        volumeMounts:
          - mountPath: /hugepages
            name: hugepage
          - name: lib-modules
            mountPath: /lib/modules
          - name: src
            mountPath: /usr/src
          - name: dev
            mountPath: /dev
          - name: cache-volume
            mountPath: /dev/shm
        resources:
          requests:
            cpu: 16
            memory: 48Gi
            hugepages-1Gi: 8Gi
          limits:
            cpu: 16
            memory: 48Gi
            hugepages-1Gi: 8Gi
      volumes:
        - name: hugepage
          emptyDir:
            medium: HugePages
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: src
          hostPath:
            path: /usr/src
        - name: dev
          hostPath:
            path: "/dev"
        - name: cache-volume
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      nodeSelector:
        node-role.kubernetes.io/worker-rt: ""

---

# apiVersion: v1
# kind: Service
# metadata:
#   name: du-http 
# spec:
#   ports:
#   - name: http
#     port: 80
#     targetPort: 80 
#   type: NodePort 
#   selector:
#     app: du-pod1

---
```

# RIC

```bash

# then create mcp, be careful, the label must be there
cat << EOF > /data/install/worker-ric.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-ric
  labels:
    machineconfiguration.openshift.io/role: worker-ric
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-ric]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-ric: ""

EOF
oc create -f /data/install/worker-ric.yaml

# to restore
oc delete -f /data/install/worker-ric.yaml

oc label node worker-1 node-role.kubernetes.io/worker-ric=""

# enable sctp
cat << EOF > /data/install/sctp-module-ric.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 99-worker-ric-load-sctp-module
  labels:
    machineconfiguration.openshift.io/role: worker-ric
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
        - path: /etc/modprobe.d/sctp-blacklist.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,
        - path: /etc/modules-load.d/sctp-load.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,sctp
EOF
oc create -f /data/install/sctp-module-ric.yaml

# intel ric needs promethus, it needs storage
# you need to craete storage for it
# for testing env in baicell, we create a nfs auto provisor
bash /data/ocp4/ocp4-upi-helpernode-master/files/nfs-provisioner-setup.sh

# intel ric will use the same network method as baicell's ran
# we use host device, it will occupy another nic,
# just for simplify
cat << EOF > /data/install/ric-net.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: host-device-intel
  namespace: intel
spec:
  config: '{
    "cniVersion": "0.3.0",
    "type": "host-device",
    "device": "eno1",
    "ipam": {
      "type": "host-local",
      "subnet": "192.168.12.0/24",
      "rangeStart": "192.168.12.167",
      "rangeEnd": "192.168.12.167",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.12.1"
    }
  }'
EOF
oc craete -f /data/install/ric-net.yaml

# then, you need to install ric soft from intel
# call intel guy to install it for you
# because the ric soft is under active development
# the install method change from time to time
# currently, it use helm to install
```

# trouble shooting
Here are commands used during trouble shooting, for reference in case needed.
```bash
# the most important, kernel.sched_rt_runtime_us should be -1, it is setting for realtime, and for stalld.
# the -1 is set by performance addon operator.
# Some SDK, like intel's, will overwrite the value to others, it will cause rhcos hang/crash/coredump.
sysctl kernel.sched_rt_runtime_us

sysctl -w kernel.sched_rt_runtime_us=-1

ps -e -o uid,pid,ppid,cls,rtprio,pri,ni,cmd | grep 'stalld\|rcuc\|softirq\|worker\|bin_read\|dumgr\|duoam' 

oc adm must-gather \
--image=registry.redhat.io/openshift4/performance-addon-operator-must-gather-rhel8 

oc get performanceprofile wzh-performanceprofile -o yaml > wzh-performanceprofile.output.yaml

oc describe node/worker-0 > node.worker-0.output

oc describe mcp/worker-rt > mcp.worker-rt.output
```
