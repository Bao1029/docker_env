# bf2 snap try

```bash
# first flash the bf2 with doca bfb image

# on 101
# config nvme storage server side
# https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/8/html/managing_storage_devices/overview-of-nvme-over-fabric-devicesmanaging-storage-devices
nmcli con modify enp5s0f1 ipv4.method manual ipv4.addresses 192.168.99.21/24
nmcli con up enp5s0f1

yum install -y nvmetcli

cd /data/down/
# wget http://git.infradead.org/users/hch/nvmetcli.git/blob_plain/0a6b088db2dc2e5de11e6f23f1e890e4b54fee64:/rdma.json
cat << EOF > /data/down/rdma.json
{
  "hosts": [
    {
      "nqn": "hostnqn"
    }
  ],
  "ports": [
    {
      "addr": {
        "adrfam": "ipv4",
        "traddr": "192.168.99.21",
        "treq": "not specified",
        "trsvcid": "4420",
        "trtype": "rdma"
      },
      "portid": 2,
      "referrals": [],
      "subsystems": [
        "testnqn"
      ]
    }
  ],
  "subsystems": [
    {
      "allowed_hosts": [],
      "attr": {
        "allow_any_host": "1"
      },
      "namespaces": [
        {
          "device": {
            "nguid": "ef90689c-6c46-d44c-89c1-4067801309a8",
            "path": "/dev/nvme0n1"
          },
          "enable": 1,
          "nsid": 1
        }
      ],
      "nqn": "testnqn"
    }
  ]
}
EOF
modprobe nvmet-rdma
nvmetcli restore /data/down/rdma.json

dmesg
# ........
# [32664.912901] nvmet: adding nsid 1 to subsystem testnqn
# [32664.914013] nvmet_rdma: enabling port 2 (192.168.99.21:4420)

yum install nvme-cli
modprobe nvme-rdma
nvme discover -t rdma -a 192.168.99.21 -s 4420
# Discovery Log Number of Records 1, Generation counter 2
# =====Discovery Log Entry 0======
# trtype:  rdma
# adrfam:  ipv4
# subtype: nvme subsystem
# treq:    not specified, sq flow control disable supported
# portid:  2
# trsvcid: 4420
# subnqn:  testnqn
# traddr:  192.168.99.21
# rdma_prtype: not specified
# rdma_qptype: connected
# rdma_cms:    rdma-cm
# rdma_pkey: 0x0000

# on 104 bf2
ovs-vsctl show
# 04d25b73-2f63-4e47-b7d9-2362cc4d7fda
#     Bridge ovsbr2
#         Port p1
#             Interface p1
#         Port en3f1pf1sf0
#             Interface en3f1pf1sf0
#         Port ovsbr2
#             Interface ovsbr2
#                 type: internal
#         Port pf1hpf
#             Interface pf1hpf
#     Bridge ovsbr1
#         Port en3f0pf0sf0
#             Interface en3f0pf0sf0
#         Port pf0hpf
#             Interface pf0hpf
#         Port p0
#             Interface p0
#         Port ovsbr1
#             Interface ovsbr1
#                 type: internal
#     ovs_version: "2.15.1"

# nmcli con modify enp3s0f0s0 ipv4.method manual ipv4.addresses 192.168.99.11/24
# nmcli con up enp3s0f0s0

# ip addr add 192.168.99.11/24 dev enp3s0f0s0 
# ip addr del 192.168.99.11/24 dev enp3s0f0s0 

cat << EOF > /etc/netplan/70-wzh-mlnx.yaml
network:
    ethernets:
        enp3s0f0s0:
            addresses:
            - 92.168.99.11/24
            dhcp4: false
    renderer: NetworkManager
    version: 2

EOF

mlxconfig -y -d /dev/mst/mt41686_pciconf0 s \
                PF_BAR2_ENABLE=0 \
                PER_PF_NUM_SF=1
mlxconfig -y -d /dev/mst/mt41686_pciconf0 s \
                PCI_SWITCH_EMULATION_ENABLE=1 \
                PCI_SWITCH_EMULATION_NUM_PORT=16 \
                VIRTIO_NET_EMULATION_ENABLE=1 \
                VIRTIO_NET_EMULATION_NUM_VF=0 \
                VIRTIO_NET_EMULATION_NUM_PF=0 \
                VIRTIO_NET_EMULATION_NUM_MSIX=16 \
                ECPF_ESWITCH_MANAGER=1 \
                ECPF_PAGE_SUPPLIER=1 \
                SRIOV_EN=0 \
                PF_SF_BAR_SIZE=8 \
                PF_TOTAL_SF=64
mlxconfig -y -d /dev/mst/mt41686_pciconf0.1 s \
                PF_SF_BAR_SIZE=10 \
                PF_TOTAL_SF=64


spdk_rpc.py bdev_nvme_attach_controller -b Nvme0 -t rdma -a 192.168.99.21 -f ipv4 -s 4420 -n testnqn
snap_rpc.py emulation_device_attach mlx5_0 virtio_blk --num_msix 8
snap_rpc.py controller_virtio_blk_create mlx5_0 --bdev_type spdk --bdev Nvme0n1 --pf_id 0 --num_queues 7


# on 104 host, rocky linux
# before
lsblk
# NAME                  MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
# sda                     8:0    0 278.9G  0 disk
# ├─sda1                  8:1    0   600M  0 part /boot/efi
# ├─sda2                  8:2    0     1G  0 part /boot
# └─sda3                  8:3    0 277.3G  0 part
#   └─rl_panlab104-root 253:0    0 277.3G  0 lvm  /

# after


```

# other
```bash
ps -ef | grep snap
# root      3986     1 99 07:07 ?        03:02:53 /usr/bin/mlnx_snap_emu -m 0xf0 -u --mem-size 1200

cd /etc/mlnx_snap

ls -hl
# total 16K
# -rw-r--r-- 1 root root 3.3K Sep  1 10:10 mlnx_snap.json
# -rw-r--r-- 1 root root  244 Sep  1 10:10 snap_rpc_init_bf1.conf
# -rw-r--r-- 1 root root  223 Sep  1 10:10 snap_rpc_init_bf2.conf
# lrwxrwxrwx 1 root root   22 Nov 24 00:46 snap_rpc_init.conf -> snap_rpc_init_bf2.conf
# -rw-r--r-- 1 root root   26 Sep  1 10:10 spdk_rpc_init.conf

cat spdk_rpc_init.conf
# bdev_malloc_create 64 512

cat snap_rpc_init.conf
# subsystem_nvme_create Mellanox_NVMe_SNAP "Mellanox NVMe SNAP Controller"
# controller_nvme_create mlx5_0 --subsys_id 0 --pf_id 0 -c /etc/mlnx_snap/mlnx_snap.json
# controller_nvme_namespace_attach -c NvmeEmu0pf0 spdk Malloc0 1

```

# nvme over fabric
https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/8/html/managing_storage_devices/overview-of-nvme-over-fabric-devicesmanaging-storage-devices